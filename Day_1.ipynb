{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## We will use a paragraph from wiki\n",
        "paragraph = \"\"\"Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\n",
        "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.[6]\"\"\""
      ],
      "metadata": {
        "id": "WTUqJpSP8RHA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "KRIQZM4I8Pnl",
        "outputId": "0a39b803-579d-4de6-e351-0445fdff9865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\\nDeep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.[6]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "paragraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing nltk libraries\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hDt8X798kP3",
        "outputId": "40ad641f-ef62-499b-e062-a3cf9270953b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9ZbxFtz9keO",
        "outputId": "42239615-ed07-44cf-b942-9aa3bbc5c156"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, or even characters\n",
        "\n",
        "#### Types of Tokenization\n",
        "1. Word Tokenization\n",
        "2. Sentence Tokenization\n",
        "3. Character Tokenization"
      ],
      "metadata": {
        "id": "lLpqP18b9YTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph) #Converting paragraph to sentences. By Default it divide by fullstop.\n",
        "\n",
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhDM42GL84OW",
        "outputId": "2203c351-8191-4835-e123-e4add071ae21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep learning is the subset of machine learning methods based on neural networks with representation learning.',\n",
              " 'The adjective \"deep\" refers to the use of multiple layers in the network.',\n",
              " 'Methods used can be either supervised, semi-supervised or unsupervised.',\n",
              " '[2]\\nDeep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.',\n",
              " '[3][4][5]\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain.',\n",
              " 'However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.',\n",
              " '[6]']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences), type(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDFhCTYI9qJv",
        "outputId": "a1161ecb-7f09-4812-ecc5-911f4eec972b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, str)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us try to do work tokenization directly to our paragraph\n",
        "\n",
        "nltk.word_tokenize(paragraph)[0:10]\n",
        "#This will show first 10 words after getting tokenized. We will again get list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwMsA-T97nW",
        "outputId": "6c4c20d0-1b56-487e-e263-a59df0a0ca59"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'the',\n",
              " 'subset',\n",
              " 'of',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'methods',\n",
              " 'based']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us see can we do same with sentences\n",
        "\n",
        "\n",
        "# nltk.word_tokenize(sentences) #If we run this code it will show an error\n",
        "\n",
        "#The error will be because nltk.word_toknize will accept the string as input\n",
        "#and sentences is having list of string.\n",
        "\n",
        "#So to overcome that we can use a loop\n",
        "\n",
        "[nltk.word_tokenize(sent) for sent in sentences][0:2] #Here the output will be\n",
        "#List of list where inner list will have all the words associated with each\n",
        "#list in sentences\n",
        "#Here we have shown just two inner list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3APzgWUW-Lna",
        "outputId": "d2328fbe-c90c-4f53-e48e-9097594ee00e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Deep',\n",
              "  'learning',\n",
              "  'is',\n",
              "  'the',\n",
              "  'subset',\n",
              "  'of',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'methods',\n",
              "  'based',\n",
              "  'on',\n",
              "  'neural',\n",
              "  'networks',\n",
              "  'with',\n",
              "  'representation',\n",
              "  'learning',\n",
              "  '.'],\n",
              " ['The',\n",
              "  'adjective',\n",
              "  '``',\n",
              "  'deep',\n",
              "  \"''\",\n",
              "  'refers',\n",
              "  'to',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'multiple',\n",
              "  'layers',\n",
              "  'in',\n",
              "  'the',\n",
              "  'network',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is the process of reducing words to their base or root form. The purpose of stemming is to normalize words to their base form so that they can be analyzed as the same item despite being different in their surface forms. For instance, the words \"running,\" \"runner,\" and \"ran\" can all be reduced to the stem \"run.\"\n",
        "\n",
        "### Types of Stemming -\n",
        "\n",
        "1. Porter Stemmer: One of the most commonly used stemming algorithms, developed by Martin Porter in 1980. It applies a series of rules to iteratively trim suffixes from words.\n",
        "\n",
        "2. Snowball Stemmer: Also known as the Porter2 stemmer, it is an improved version of the Porter Stemmer and provides more accurate results.\n",
        "\n",
        "3. Lancaster Stemmer: Known for being more aggressive compared to the Porter Stemmer. It can result in very short stems and might be too aggressive for some applications.\n",
        "\n",
        "\n",
        "\n",
        "##### We have another option Lemmatization\n",
        "\n",
        "### What is Lemmatization -\n",
        "Lemmatization is the process of reducing a word to its base or dictionary form, known as a lemma. Unlike stemming, which simply cuts off word endings in an attempt to achieve a base form, lemmatization considers the context and converts the word to its meaningful base form. Lemmatization is more sophisticated than stemming and usually produces more accurate results. Lemmatization is highly beneficial for tasks requiring a deeper understanding of the text, such as sentiment analysis, text classification, and more.\n",
        "\n",
        "\n",
        "\n",
        "#### BUT AS OF NOW WE WILL USE STEMMING"
      ],
      "metadata": {
        "id": "LgtmFU64_yr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will use stemming - We will use Snowball Stemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english') #Defined the object\n"
      ],
      "metadata": {
        "id": "lSAzO2XC-m0u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_Stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "vpPHtkowAZeg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We need to clean the data too, we will use regex for that\n",
        "\n",
        "import re\n",
        "\n",
        "corpus = []\n",
        "\n",
        "#Doing stemming\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "-zSARn17ArK_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s-3E_89_CDTm",
        "outputId": "aa91ac44-f66b-4c11-a695-2763d9d545e6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'earli form neural network inspir inform process distribut communic node biolog system particular human brain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Mq1pdu2sCQJX",
        "outputId": "a76ad7eb-3b0b-498a-e433-9f08f48b656f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[3][4][5]\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words\n",
        "\n",
        "The Bag of Words (BoW) model is a fundamental technique in natural language processing (NLP) for representing text data. It simplifies the representation of text by treating it as a collection of words (or tokens), disregarding grammar and word order but maintaining the multiplicity of occurrences. Each unique word in the text is treated as a feature.\n",
        "\n",
        "### How Bag of Words Works\n",
        "\n",
        "1. **Text Preprocessing:**\n",
        "   - Tokenize the text into words.\n",
        "   - Convert all words to lowercase (or uppercase) to ensure uniformity.\n",
        "   - Remove punctuation, special characters, and stopwords (common words like \"the,\" \"is,\" etc. that do not carry significant meaning).\n",
        "\n",
        "2. **Vocabulary Creation:**\n",
        "   - Create a vocabulary of all unique words present in the text.\n",
        "\n",
        "3. **Vector Representation:**\n",
        "   - For each document or sentence, create a vector of length equal to the vocabulary size.\n",
        "   - Each element of the vector represents the count of a word's occurrence in the document.\n",
        "\n",
        "### Example\n",
        "\n",
        "Consider the following two sentences:\n",
        "1. \"I love NLP.\"\n",
        "2. \"NLP is fun.\"\n",
        "\n",
        "#### Step-by-Step Process:\n",
        "\n",
        "1. **Tokenization and Preprocessing:**\n",
        "   - Sentence 1: ['i', 'love', 'nlp']\n",
        "   - Sentence 2: ['nlp', 'is', 'fun']\n",
        "\n",
        "2. **Vocabulary Creation:**\n",
        "   - Vocabulary: ['i', 'love', 'nlp', 'is', 'fun']\n",
        "\n",
        "3. **Vector Representation:**\n",
        "   - Sentence 1: [1, 1, 1, 0, 0]\n",
        "   - Sentence 2: [0, 0, 1, 1, 1]\n",
        "\n",
        "\n",
        "### Considerations\n",
        "\n",
        "- **Simplicity:** The BoW model is simple and easy to understand but does not capture the semantics (meaning) of the words or the order of words in the text.\n",
        "- **High Dimensionality:** For large text corpora, the vocabulary can become very large, leading to high-dimensional feature vectors.\n",
        "- **Sparsity:** Most vectors will be sparse (i.e., containing many zeros) because most documents contain only a small subset of the total vocabulary.\n",
        "\n",
        "### Applications\n",
        "\n",
        "- **Text Classification:** BoW is commonly used in text classification tasks where the goal is to classify documents into predefined categories.\n",
        "- **Information Retrieval:** Used in search engines to index and retrieve documents based on keyword queries.\n",
        "- **Document Clustering:** Helps in clustering similar documents together.\n",
        "\n",
        "The Bag of Words model is a foundational concept in NLP and serves as a stepping stone to more advanced text representation techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings (e.g., Word2Vec, GloVe).\n"
      ],
      "metadata": {
        "id": "Z6ocFSJGC-Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "OfGTcxj4CpLF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus) #X is a sparse matrix"
      ],
      "metadata": {
        "id": "1BXt_ZfsDk96"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will check index value of each vocabulary\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcSC17T_D1-J",
        "outputId": "1159a522-68c5-4be5-d649-885996a2ddaa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'deep': 17,\n",
              " 'learn': 39,\n",
              " 'subset': 68,\n",
              " 'machin': 41,\n",
              " 'method': 44,\n",
              " 'base': 4,\n",
              " 'neural': 49,\n",
              " 'network': 48,\n",
              " 'represent': 62,\n",
              " 'adject': 0,\n",
              " 'refer': 61,\n",
              " 'use': 75,\n",
              " 'multipl': 46,\n",
              " 'layer': 38,\n",
              " 'either': 22,\n",
              " 'supervis': 69,\n",
              " 'semi': 66,\n",
              " 'unsupervis': 74,\n",
              " 'architectur': 3,\n",
              " 'belief': 5,\n",
              " 'recurr': 60,\n",
              " 'convolut': 15,\n",
              " 'transform': 72,\n",
              " 'appli': 2,\n",
              " 'field': 24,\n",
              " 'includ': 32,\n",
              " 'comput': 14,\n",
              " 'vision': 76,\n",
              " 'speech': 67,\n",
              " 'recognit': 59,\n",
              " 'natur': 47,\n",
              " 'languag': 37,\n",
              " 'process': 54,\n",
              " 'translat': 73,\n",
              " 'bioinformat': 6,\n",
              " 'drug': 20,\n",
              " 'design': 18,\n",
              " 'medic': 43,\n",
              " 'imag': 31,\n",
              " 'analysi': 1,\n",
              " 'climat': 11,\n",
              " 'scienc': 64,\n",
              " 'materi': 42,\n",
              " 'inspect': 34,\n",
              " 'board': 8,\n",
              " 'game': 27,\n",
              " 'program': 56,\n",
              " 'produc': 55,\n",
              " 'result': 63,\n",
              " 'compar': 13,\n",
              " 'case': 10,\n",
              " 'surpass': 70,\n",
              " 'human': 30,\n",
              " 'expert': 23,\n",
              " 'perform': 53,\n",
              " 'earli': 21,\n",
              " 'form': 25,\n",
              " 'inspir': 35,\n",
              " 'inform': 33,\n",
              " 'distribut': 19,\n",
              " 'communic': 12,\n",
              " 'node': 50,\n",
              " 'biolog': 7,\n",
              " 'system': 71,\n",
              " 'particular': 52,\n",
              " 'brain': 9,\n",
              " 'howev': 29,\n",
              " 'current': 16,\n",
              " 'intend': 36,\n",
              " 'model': 45,\n",
              " 'function': 26,\n",
              " 'organ': 51,\n",
              " 'general': 28,\n",
              " 'seen': 65,\n",
              " 'low': 40,\n",
              " 'qualiti': 58,\n",
              " 'purpos': 57}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ISLWmDj1D63C",
        "outputId": "8eade861-b6e0-42f6-eea8-8df713e84e7c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deep learn subset machin learn method base neural network represent learn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#index of deep is 17\n",
        "#Let us see how the first courpus that is at 0th index has been created as bag of words\n",
        "\n",
        "X[0].toarray()\n",
        "#If you see there will be 1 on 17th index, that means index of deep is 17 at each list and it is repeatd 1 time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qktJMUUHEEKI",
        "outputId": "41be5502-602e-4f45-ef11-75fa2ef94850"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0,\n",
              "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QtVawmGEU_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}